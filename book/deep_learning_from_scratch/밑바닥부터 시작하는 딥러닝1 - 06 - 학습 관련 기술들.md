# 밑바닥부터 시작하는 딥러닝1 - 06 - 학습 관련 기술들

## 매개변수 갱신

최적화(optimization) : 손실 함수의 값을 가능한 낮추는 매개변수를 찾는 것, 즉 매개변수의 최적값을 찾는 것

#### 확률적 경사 하강법(SGD)

W <- W - (학습률 * 손실 함수의 기울기), 이전 챕터에서 봐서 익숙할 수도 있는 그 접근법이다.

단점 = 비등방성 함수(방향에 따라 성질(기울기)가 달라지는 함수)에는 매우 비효율적이다.

#### 모멘텀(Momentum)

v <- av - (학습률 * 손실함수의 기울기)

W <- W + v

기울기에 따라 물체의 속도가 달라지는 것을 응용한 공식, v가 속도, a가 표면마찰의 역할을 수행한다.

SGD보다 x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어듬.

#### AdaGrad

h <- h + (손실함수의 기울기 ** 2)

W <- W - (학습률 * `(1 / h의 루트 제곱근)` * 손실함수의 기울기)

각각의 매개변수에 맞춤형 학습률 감소를 적용하는 알고리즘, 이 계산 공식이 원소별로 다르게 적용되기 때문에 크게 갱신된 원소는 h값에 의해서 학습률이 낮아진다.

#### Adam

`모멘텀 + AdaGrad + 하이퍼파라미터의 편향 보정 = Adam`

<img src="./img/skip.jpeg">



### 위의 알고리즘들 모두 장단점이 있기에 필요에 따라 알아서 사용



## 가중치의 초깃값

오버피팅을 막기 위해 가중치를 최대한 낮은 값부터 시작하는 것을 `가중치 감소(weight decay) 기법` 이라 한다.

그런데 왜 가중치를 낮은 값부터 시작해야 하는거죠? 활성화값 부분의 미분이 0에 가까워지면서 역전파의 기울기 값이 점점 작아지다가 사라지는 `기울기 소실(gradient vanishing)`이 발생하기 때문, 이는 표현력을 제한다기 때문에 문제가 된다. (쉽게 말해서 활성화 값들의 분포가 점점 치우져서 정상적인 학습이 불가능해지는 것)

가중치를 0으로 해도 되요?  안돼! (역전파 때 배운거 생각해보자)

가장 알려진 가중치 초깃값으로는 선형 활성화 함수(sigmoid, tanh 등등)을 위한 `Xavier 초깃값`, Relu 함수를 위한 `He 초깃값`이다.



## 배치 정규화

2015년에 나온 개념으로 각 층이 활성화를 적당히 퍼트리도록 강제하는 아이디어에서 시작

장점 : 학습 속도 개선, 초깃값에 크게 의존하지 않음, 오버피팅 억제(드롭아웃 등의 필요성 감소)



## 바른 학습을 위해

오버피팅 : 신경망이 훈련 데이터'에만' 지나치게 적응되어 다른 데이터에 대응하지 못하는 상태

이와 같은 현상은 아래와 같은 경우에 주로 발생한다.

1. 매개변수가 많고 표현력이 높은 모델
2. 훈련 데이터가 적음



이를 막는 방법 중의 대표적으로는 아래의 기법이 있다.

1. 가중치 감소
2. 드롭 아웃

가중치 감소: 큰 가중치에는 상응하는 패널티를 부과하여 오버피팅을 억제하는 방법

드롭 아웃: 무작위 뉴런을 비활성화하면서 학습을 진행하는 방식. 물론 실제 Test 때에는 학습에 참여하지 않았던 뉴런도 활용한다. (단 각 뉴런의 출력에 훈련 때 삭제한 비율을 곱할 것)



## 적절한 하이퍼파라미터 값 찾기

하이퍼파라미터 값의 평가를 위해 train data, test data에 validation data(검증 데이터)를 추가할 것. 단 validation data에는 train data를 약간 분할하여(얼추 20퍼센트) 할당

1. 하이퍼파라미터 값의 범위를 설정
2. 설정한 범위에서 하이퍼파라미터의 값을 무작위로 추출
3. 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고 검증 데이터로 정확도를 평가 (epoch은 적게)
4. 1,2단계를 특정 횟수만큼 반복하고 그 정확도와 결과를 보고 하이퍼파라미터의 범위를 축소



<img src="./img/mining.jpg">

#### 세련되고 과학적인 hyperparameter optimization을 위해 Bayesian Optimization을 추가로 학습하자(책에서는 언급만 되는 정도)