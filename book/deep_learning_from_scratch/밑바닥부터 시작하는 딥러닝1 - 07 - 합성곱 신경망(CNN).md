# 밑바닥부터 시작하는 딥러닝1 - 07 - 합성곱 신경망(CNN)

## 전체 구조

<img src="./img/cnn_model.jpeg">

기존의 `Affine - Relu` 대신 `Conv - Relu - (Pooling)`의 흐름으로 변경된다. 물론 feature learning에 해당하는 부분으로 classfication 전후에는 `Affine - Relu`를 사용하기도 하며 마지막 분류에서는 `Affine-Softmax`의 형태를 보여준다.



## 합성곱 계층

지난 챕터에서 배웠던 `완전연결 계층(Affine 계층)` 다차원의 데이터를 1차원으로 flatten 시켜야 학습이 가능하다. 그렇기 때문에 데이터의 형상을 무시하게 되고 그 안에 담긴 정보를 살릴 수가 없다. CNN은 3차원의 input이 3차원의 output으로 전달하기 때문에 데이터를 제대로 이해할 가능성을 가지게 된다. 

특징 맵(feature map) : 합성곱 계층의 입출력 데이터

<img src="./img/conv-layer.png">

필터의 매개변수에 맞춰서 합성곱 연산하여 결과를 전달한다. 필터의 윈도우를 일정 간격으로 이동시키면서 입력 데이터에 적용한다.

패딩(padding) : 입력 데이터 주변을 특정 값(0이나 다른 값)으로 채우는 것

스트라이드(stride) : 필터가 적용하는 위치의 간격

위와 같은 연산을 입력 데이터의 채널 수와 필터의 채널 수를 맞춰가면서 수행한다. 예를 들자면 RGB로 색상을 구현한 이미지 파일의 경우에는 채널의 수가 RGB에 맞춰 3개이기 때문에 필터의 채널 수도 이에 맞춰 3개로 구성해야 한다.



입력값을 복수의 필터에 통과시켜 2차원의 필터 개수만큼의 출력값을 얻는다. 이를 통해 3(2+1)차원을 가진 데이터의 성질을 유지시킨다.

`입력데이터(C,H,W) * 필터[FN,C,FH,FW] = 필터처리된 데이터(FN, OH, OW)`

`필터처리된 데이터(FN, OH, OW) + 편향(FN, 1, 1) = 출력 데이터(FN, OH, OW)`



## 풀링 계층

풀링은 세로 또는 가로 방향의 공간을 줄이는 연산

1. 학습해야 하는 매개변수가 없다
2. 채널 수가 변하지 않는다
3. 입력의 변화에 영향을 적게 받는다(강건함)



## 대표적인 CNN

#### LeNet

현재 일반적인 CNN과의 차이

1. ReLU 대신 Sigmoid를 썼음
2. 최대 풀링을 하지 않고 서브샘플링을 해서 중간 데이터의 크기를 줄였음

#### AlexNet

LeNet과의 차이

1. 활성화 함수로 ReLU를 씀
2. LRN이라는 국소적 정규화를 실시하는 계층을 이용
3. 드롭아웃을 사용