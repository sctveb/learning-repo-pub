# 밑바닥부터 시작하는 딥러닝1 - 03 - 신경망

## 퍼셉트론에서 신경망으로

신경망은 퍼셉트론과 유사하지만 다른 특징을 가지고 있다. 입력층과 출력층 사이에 은닉층이라는 보이지 않는 층이 존재한다. 또한 기존의 입력 값, 가중치, 편향 뿐만 아니라 활성화 함수(h(x))가 등장한다. 활성화 함수란 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 하는 함수이다.

a = w1x1 + w2x2

y = h(a)



## 활성화 함수

활성화 함수는 임계값을 경계로 출력이 바뀌는데 이런 함수들을 계단 함수라고 한다.

#### 단위 계단 함수

<img src="./img/heaviside-step-function.png">

#### 시그모이드 함수

<img src="./img/sigmoid-function.png">

위의 단위 계단 함수와 다르게 시그모이드 함수는 매끄러운 곡선이며 입력값에 따라 연속적으로 변화함. 하지만 입력값이 0 또는 1이라는 하나의 결과로 수렴한다는 점에서 기존의 계단 함수의 역할을 충분히 수행한다.

#### ReLU 함수

<img src="./img/relu-function.png">

최근에 시그모이드 함수를 대체할 활성화 함수로 사용되는 ReLU 함수이다.




위 함수들의 공통점으로 비선형 함수라는 점이 있다. 선형 함수를 활성화 함수로 사용할 경우, 은닉층이 없는 네트워크로도 표현이 가능하기 때문에 여러 층의 은닉층으로 쌓는 의미가 사라진다.  그렇기 때문에 활성화 함수로는 비선형 함수를 사용한다.



## 다차원 배열의 계산

```python
import numpy as np
A = np.array([[1,2,3], [4,5,6]])
A.shape # (2,3)
B = np.array([[1,2], [3,4], [5,6]])
B.shape # (3,2)
np.dot(A, B)
'''
array([[22, 28],
       [49, 64]])
'''
```

행결의 곱을 할 때에는 대응하는 차원의 원소 수를 일치시켜야 한다. 예제의 경우,

(2,3) * (3,2) = (2,2) 로 첫번째 배열의 열과 두번째 배열의 행이 같다.



## 3층 신경망 구현하기



## 출력층 설계하기



## 손글씨 숫자 인식