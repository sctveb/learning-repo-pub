# 밑바닥부터 시작하는 딥러닝1 - 05 - 오차역전파법

## 계산 그래프

일반적으로 오차역전파법을 설명하는데 수식이나 계산 그래프 중 하나를 택해서 설명하는데 이 책에서는 직관적인 설명을 위해 계산 그래프를 활용했다. 이를 통해 매우 중요한 개념을 함께 설명한다. 

#### 국소적 계산

계산 그래프를 통한 접근은 복잡한 계산도 관련된 작은 단위로 쪼개서 계산할 수 있다는 것을 보여준다. 이를 국소적 계산이라고 하며 오차역전파법이 사용하는 미분을 효율적으로 계산할 수 있게 한다.



순전파에서 x를 f(x)로 집어넣어 y를 얻었을 때, 역전파에서는 반대로 E(신호)에 y에 대한 x의 미분값을 곱해서 역으로 전달한다. 이를 효과적으로 해내기 위해서는 `미분의 연쇄법칙`에 대한 이해가 동반된다.

## 연쇄법칙

`합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다`

<img src="./img/wut.jpeg">

```python
# 다음과 같은 함수가 있다고 가정하자
z = t ** 2
t = x + y
# x에 대한 z의 미분 = t에 대한 z의 미분 * x에 대한 t의 미분
## round z / round x = round z / round t * round t / round x
```

짧게 말하자면 `합성함수는 쪼개서 곱셈의 형태로 쉽게 미분을 계산할 수 있다`는 것이다.



## 역전파

덧셈 노드의 역전파 : z = x + y라고 한다면 x에 대한 z의 미분, y에 대한 z의 미분은 전부 1이 된다. 이전에 받은 연산을 그대로 전달한다.

곱셈 노드의 역전파 : z =xy 라고 한다면 x에 대한 z의 미분은 y, y에 대한 z의 미분은 x가 된다. 이전에 받은 연산에 서로 바꾼 값을 곱해 전달한다.



## 활성화 함수 계층 구현하기

Relu 계층의 역전파: x가 0보다 크면 x를 반환하고 그렇지 않으면 0을 반환하는 함수의 특성상, 역전파는 순전파 때 입력값이 0보다 크면 그대로, 그렇지 않으면 0을 전달한다.

Sigmoid 계층의 역전파: `1 / (1 + exp(-x))` 라는 공식을 한꺼번에 계산하기 어렵기에 아래와 같이 쪼개서 계산한다.

`x => (* -1) => exp => (+ 1) => (/)` = sigmoid

긴 설명을 생략하고 역전파로 나온 값은 `E(이전 신호) * y(1-y)`가 된다.



## Affine/Softmax 계층 구현하기

Affine 계층 : 이전에 `y = Wx + b` 라는 공식을 기억하는가? 가중치(weight)와 편향(bias)을 이용해 퍼셉트론과 신경망 학습을 구현하는데 사용된 공식이다. 이런 형태의 연산(행렬의 곱)을 어파인 변환(affine transformation)이라 한다. 이에 맞춰 위 공식과 같은 구조를 가진 계층을 `Affine 계층`이라 부른다. 

실제 구현은 행렬의 형태를 고려해서 위에서 했던 연산을 응용하면 된다.

Softmax-with-Loss 계층 : 출력층에서 입력값을 정규화하여 출력해주는 Softmax와 손실 함수인 교차 엔트로피 오차를 합친 계층, 책에서도 설명하면 무지 복잡해져서 구체적인 설명은 부록으로 넘겨놓고 결과만 서술.

실제로 각 라벨별로 전달되는 역전파는 `yn - tn`, 즉 `softmax 계층의 출력과 정답 레이블의 차분`이다.

